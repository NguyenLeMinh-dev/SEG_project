import torch
from transformers import AutoModel
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
from tqdm import tqdm
import os

# ==============================================================================
# SECTION 1: CONSTANTS AND CONFIGURATION
# ==============================================================================

# --- Input/Output Files ---
INPUT_TENSORS = "tokenized_data.pt"
OUTPUT_EMBEDDINGS = "embeddings.npy"

# --- Model & Hardware Configuration ---
MODEL_NAME = "vinai/phobert-base"
# Increase batch size for powerful GPUs like the 4060 Ti to maximize performance
BATCH_SIZE = 128 

# ==============================================================================
# SECTION 2: MAIN PROCESSING FUNCTION
# ==============================================================================

def generate_embeddings():
    """
    Main function to run the embedding generation pipeline.
    This is the most computationally intensive step.
    """
    # --- 1. Setup device (GPU/CPU) ---
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"‚úÖ S·ª≠ d·ª•ng thi·∫øt b·ªã: {device}")
    if device.type == 'cpu':
        print("‚ö†Ô∏è  C·∫£nh b√°o: Ch·∫°y tr√™n CPU s·∫Ω ch·∫≠m h∆°n ƒë√°ng k·ªÉ. H√£y c√¢n nh·∫Øc d√πng Google Colab.")

    # --- 2. Load tokenized data ---
    try:
        tokenized_data = torch.load(INPUT_TENSORS)
        dataset = TensorDataset(tokenized_data['input_ids'], tokenized_data['attention_mask'])
        dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)
        print(f"üíæ ƒê√£ t·∫£i d·ªØ li·ªáu ƒë√£ tokenize t·ª´ '{INPUT_TENSORS}'.")
    except FileNotFoundError:
        print(f"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file '{INPUT_TENSORS}'. Vui l√≤ng ch·∫°y 'Tokenize.py' tr∆∞·ªõc.")
        return

    # --- 3. Load the PhoBERT model ---
    print(f"ü§ñ ƒêang t·∫£i m√¥ h√¨nh PhoBERT ('{MODEL_NAME}')...")
    try:
        # Move model to the selected device (GPU)
        model = AutoModel.from_pretrained(MODEL_NAME).to(device)
        # Set to evaluation mode to disable dropout and save memory
        model.eval()
        print("‚úÖ T·∫£i m√¥ h√¨nh th√†nh c√¥ng.")
    except Exception as e:
        print(f"‚ùå L·ªói khi t·∫£i m√¥ h√¨nh: {e}. Vui l√≤ng ki·ªÉm tra k·∫øt n·ªëi m·∫°ng.")
        return
        
    # --- 4. Generate Embeddings in Batches ---
    all_embeddings = []
    print(f"\n‚öôÔ∏è  B·∫Øt ƒë·∫ßu t·∫°o embeddings v·ªõi batch size = {BATCH_SIZE}...")
    
    # Disable gradient calculation to save VRAM and speed up inference
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Generating Embeddings"):
            # Move data batch to the GPU
            b_input_ids, b_attention_mask = [b.to(device) for b in batch]
            
            # Get model outputs
            outputs = model(input_ids=b_input_ids, attention_mask=b_attention_mask)
            
            # Extract the [CLS] token's embedding, which represents the entire sentence
            cls_embeddings = outputs.last_hidden_state[:, 0, :]
            
            # Move embeddings back to CPU and convert to NumPy array
            all_embeddings.append(cls_embeddings.cpu().numpy())

    # Combine embeddings from all batches into a single NumPy matrix
    final_embeddings = np.concatenate(all_embeddings, axis=0)
    print("‚úÖ T·∫°o embeddings ho√†n t·∫•t!")
    print(f"K√≠ch th∆∞·ªõc c·ªßa ma tr·∫≠n embeddings: {final_embeddings.shape}")

    # --- 5. Save the final embeddings ---
    try:
        np.save(OUTPUT_EMBEDDINGS, final_embeddings)
        print(f"\nüíæ ƒê√£ l∆∞u embeddings v√†o file: '{OUTPUT_EMBEDDINGS}'")
        print("üëâ B∆∞·ªõc ti·∫øp theo: D√πng file n√†y ƒë·ªÉ ch·∫°y 'search_engine.py'.")
    except Exception as e:
        print(f"‚ùå L·ªói khi l∆∞u file: {e}")

# ==============================================================================
# SECTION 3: SCRIPT EXECUTION
# ==============================================================================

if __name__ == "__main__":
    generate_embeddings()
